# Parallel-Linear-Algebra
[Более подробное описание задачи](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/task.pdf)

## 1. Реализовать алгоритмы для умножения матрицы на вектор, используя разбиение по строкам, по столбцам и по блокам.

### Описание задачи

На вход подаётся матрица $A$ размера $I x J$ и вектор $B$. Нам требуется умножить $A * B$. Результатом является вектор.

Три способа умножения матрицы на вектор:
* По строкам: умножаем набор строк матрицы на набор компонент вектора $B$
* По столбцам: то же, что и по строкам, но по столбцам. В конце выполнения работы каждого процесса требуется просуммировать результаты в компонентах итогового вектора
* По блокам: матрица делится на 2D блоки. В каждом блоке матрицы перемножаются по строкам (или столбцам, по желанию)

### Решение задачи

В случае умножения по строкам мы даём равное непрерывное количество строк матрицы процессам. После умножения объединяем локальные результаты.

При умножении по столбцам мы шарим матрицу на все процессы, потому что столбцы в памяти расположены не последовательно. 
После умножения суммируем получившиеся локальные вектора в один.

Блочное умножение предлагается сделать, как описано в учебнике Гергеля: будем делить матрицу квадратной сеткой $N x N$ блоков, в каждой ячейке которой будет количество строк $I / N$ и количество столбцов $J / N$.
Далее блоки умножаются на соответствующую часть вектора по строкам. 
Так как сетка квадратная, то количество задействованных процессов в умножении матрицы на вектор будет равнятся корню ближайшего снизу квадрата (1, 4, 9, 16, ...). 
То есть процессов 5, то мы создадим сетку на $2 x 2$ блоков.

Полная реализация алгоритмов, находится в [main](https://github.com/Raincwail/Parallel-Linear-Algebra/tree/master/GEMV-MPI). 
В файле gemv_rows.cpp реализовано умножение по строкам, в gemv_cols.cpp - по столбцам, в gemv_block.cpp - по блокам. 
Так же там есть скрипт для замеров производительности (см. README в GEMV-MPI).

### Оценка качества

Замеры производились 100 итераций умножений, далее время усреднялось. 
Есть погрешности, например, в некоторых случаях эффективность выше 1.

Замеры производительности в абсолютных числах даны в [GEMV-MPI](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/GEMV-MPI/README.md). 
В папке [images](https://github.com/Raincwail/Parallel-Linear-Algebra/tree/master/GEMV-MPI/images) содержатся графики ускорения алгоритмов. Ускорение и эффективность в относительных числах посчитаны ниже. 
Три размера матриц: 1000x1000, 5000x5000, 10000x10000

#### Умножение по строкам:

> Матрица 1000x1000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 2 | 1 |
| 3 | 3.04 | 1.01 |
| 4 | 3.97 | 0.99 |
| 5 | 4.96 | 0.99 |
| 6 | 6.09 | 1.01 |
| 7 | 7.05 | 1.01 |
| 8 | 7.89 | 0.99 |

> Матрица 5000x5000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 2.05 | 1.02 |
| 3 | 3.06 | 1.02 |
| 4 | 4.05 | 1.01 |
| 5 | 4.86 | 0.97 |
| 6 | 5.56 | 0.93 |
| 7 | 5.96 | 0.85 |
| 8 | 6.64 | 0.83 |

> Матрица 10000x10000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 2 | 1 |
| 3 | 3 | 1 |
| 4 | 3.94 | 0.98 |
| 5 | 4.66 | 0.93 |
| 6 | 5.43 | 0.9 |
| 7 | 5.81 | 0.83 |
| 8 | 6.1 | 0.76 |

#### Умножение по столбцам:

> Матрица 1000x1000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 1.97 | 0.99 |
| 3 | 3.01 | 1 |
| 4 | 4.01 | 1 |
| 5 | 4.14 | 0.83 |
| 6 | 5.92 | 0.99 |
| 7 | 6.6 | 0.94 |
| 8 | 7.92 | 0.99 |

> Матрица 5000x5000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 1.99 | 0.99 |
| 3 | 2.83 | 0.94 |
| 4 | 3.69 | 0.92 |
| 5 | 4.26 | 0.85 |
| 6 | 5.03 | 0.84 |
| 7 | 6.11 | 0.87 |
| 8 | 6.57 | 0.82 |

> Матрица 10000x10000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 2 | 1.97 | 0.98 |
| 3 | 2.92 | 0.97 |
| 4 | 3.7 | 0.92 |
| 5 | 4.37 | 0.87 |
| 6 | 4.77 | 0.79 |
| 7 | 4.74 | 0.68 |
| 8 | 4.85 | 0.61 |

#### Умножение по блокам:

> Матрица 1000x1000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 4 | 3.81 | 0.95 |
| 9 | 8.17 | 0.91 |
| 16 | 13.91 | 0.87 |

> Матрица 5000x5000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 4 | 3.76 | 0.94 |
| 9 | 5.65 | 0.63 |
| 16 | 6.4 | 0.4 |

> Матрица 10000x10000:

| N процессов | Ускорение | Эффективность |
| ------------- | ------------- | ------------- |
| 1 | 1 | 1 |
| 4 | 3.8 | 0.95 |
| 9 | 5.8 | 0.64 |
| 16 | 6.42 | 0.4 |

## 2. Реализовать алгоритм матричного умножения с разбиением на блоки по алгоритму Кэннона.

### Описание задачи

На вход подаются матрицы $A$ и $B$, а также число процессов $procNum$. В качестве результат алгоритм выдает матрицу $C = A * B$.

Ограничения:
* $A$ и $B$ квадратные матрицы
* $procNum$ должен быть квадратом какого-либо числа
* Корень $procNum$ должен быть делителем размера матриц

### Решение задачи

Алгоритм представляет из себя разделение матриц на блоки в соответствии с количество блоков для последующего перемножения соответствующих блоков.

Предположим, что матрицы можно разделить на 4 блока, тогда процессы будут распологаться следующим образом:
<div align="center"><table>
<tr><th> Матрица А </th><th> Матрица B </th></tr>
<tr><td>

| P00(A) | P01(A) |
|--|--|
| P10(A)| P11(A) |

</td><td>

| P00(B) | P01(B) |
|--|--|
| P10(B)| P11(B) |

</td></tr> </table> </div>

Затем каждый процесс вычисляет свою часть $C$, например, $C00 = P00(A) * P00(B)$.

Далее производятся сдвиги: матрица $A$ сдвигается влево, матрица $B$ сдвигается вверх.

<div align="center"><table>
<tr><th> Матрица А </th><th> Матрица B </th></tr>
<tr><td>

| P01(A) | P00(A) |
|--|--|
| P11(A)| P10(A) |

</td><td>

| P10(B) | P11(B) |
|--|--|
| P00(B)| P01(B) |

</td></tr> </table> </div>

Вновь высчитывается произведение, но на этот раз результат прибавляется к существующему, т.е. $C00 += P01(A) * P10(B)$.

Операция повторяется $sqrt(procNum)$ раз.

Полная реализация алгоритма, включая установку матриц, находится в [main](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CANNON-MPI/main.c).

### Оценка качества

В качестве оценки качества решения были проведены следующие эксперименты и получены соответствующие результаты (мс).

| Размер Матрицы / Версия Решения | 1024x1024 | 2048x2048 | 4096x4096 |
| ------------- | ------------- | ------------- | ------------- |
| Параллельная версия (1 поток) | 4302 | 121618 | 1107304 |
| Параллельная версия (4 потока) | 1449 | 18542 | 337624 |
| Параллельная версия (16 потоков) | 989 | 8760 | 115023 |
| Параллельная версия (64 потока) | 1615 | 8995 | 69960 |

Более наглядные графики, а также таблицы анализа ускорения и эффективности можно найти в ноутбуке [analysis](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CANNON-MPI/analysis.ipynb).

## 3.  С использованием CUDA решить СЛАУ прямым (Гаусс) или итерационным методом (CG, сопряженные градиенты).

### Описание задачи

На вход подается симметричная, положительно определенная матрица $A$ и вектор $b$, в качестве результат алгоритм выдает решение СЛАУ $Ax = b$.

Алгоритм представляет из себя итеративный градиентный спуск с минимизацией  $\phi(x) = \frac{1}{2}x^T A x - x^T b$.

### Решение задачи

В качестве решения были реализованы [итеративная](https://github.com/Raincwail/Parallel-Linear-Algebra/tree/master/CG-CUDA/iterative) и [параллельная](https://github.com/Raincwail/Parallel-Linear-Algebra/tree/master/CG-CUDA/parallel) версии.

Для установки матрицы $A$ и вектора $b$ используется заголовок [Base](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CG-CUDA/Base.h).

Основное отличие заключается в использованных векторных операциях - итеративная версия использует наивные имплементации, в то время как параллельная использует инструменты CUDA для ускорения вычислений.

### Оценка качества

В качестве оценки качества решения были проведены следующие эксперименты и получены соответствующие результаты (мс).

| Размер Матрицы / Версия Решения | 512x512 | 1024x1024 | 2048x2048 | 4096x4096 |
| ------------- | ------------- | ------------- | ------------- | ------------- |
| Итеративная версия | 639 | 2518 | 9892 | 38988 |
| Параллельная версия (16 потоков) | 89 | 194 | 548 | 1470 |
| Параллельная версия (32 потока) | 77 | 149 | 325 | 752 |
| Параллельная версия (64 потока) | 92 | 127 | 307 | 745 |

Для замеров времени использовались [Timer](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CG-CUDA/iterative/Timer.h) и [GpuTimer](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CG-CUDA/parallel/GpuTimer.cuh) соответственно.

Более наглядные графики, а также таблицы анализа ускорения и эффективности можно найти в ноутбуке [analysis](https://github.com/Raincwail/Parallel-Linear-Algebra/blob/master/CG-CUDA/analysis.ipynb).
